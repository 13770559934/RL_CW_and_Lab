{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gt7Z9F2urDvb"
      },
      "outputs": [],
      "source": [
        "# Instaling in Google Colab the libraries used for this assignemnt\n",
        "# Don't run this cell if you are running the notebook locally\n",
        "\n",
        "# !pip install pygame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "button": false,
        "id": "20IyxDzgp3tU",
        "new_sheet": false,
        "run_control": {
          "read_only": false
        }
      },
      "outputs": [],
      "source": [
        "# Importing the libraries\n",
        "\n",
        "import os\n",
        "import gymnasium as gym\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt # Graphical library\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\") # Configuring Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNPHHkRcUTLg"
      },
      "source": [
        "# Solution of Lab Assignment 3 :  \n",
        "See pdf for instructions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q06EldpwfoEB"
      },
      "source": [
        "## Part 1: Introduction to Gym environments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl3dm2KOPrfi"
      },
      "source": [
        "### Question 1: Creating the Cartpole environment and performing an episode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QzdwlOmsfydj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function VideoRecorder.__del__ at 0x7fac48ac7a60>\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/ruihongyu/opt/anaconda3/lib/python3.8/site-packages/gym/wrappers/monitoring/video_recorder.py\", line 178, in __del__\n",
            "    self.close()\n",
            "  File \"/Users/ruihongyu/opt/anaconda3/lib/python3.8/site-packages/gym/wrappers/monitoring/video_recorder.py\", line 158, in close\n",
            "    clip.write_videofile(self.path)\n",
            "  File \"/Users/ruihongyu/opt/anaconda3/lib/python3.8/site-packages/decorator.py\", line 231, in fun\n",
            "    es = ''\n",
            "  File \"/Users/ruihongyu/opt/anaconda3/lib/python3.8/site-packages/moviepy/decorators.py\", line 54, in requires_duration\n",
            "    return f(clip, *a, **k)\n",
            "  File \"/Users/ruihongyu/opt/anaconda3/lib/python3.8/site-packages/decorator.py\", line 231, in fun\n",
            "    es = ''\n",
            "  File \"/Users/ruihongyu/opt/anaconda3/lib/python3.8/site-packages/moviepy/decorators.py\", line 135, in use_clip_fps_by_default\n",
            "    return f(clip, *new_a, **new_kw)\n",
            "  File \"/Users/ruihongyu/opt/anaconda3/lib/python3.8/site-packages/decorator.py\", line 231, in fun\n",
            "    es = ''\n",
            "  File \"/Users/ruihongyu/opt/anaconda3/lib/python3.8/site-packages/moviepy/decorators.py\", line 22, in convert_masks_to_RGB\n",
            "    return f(clip, *a, **k)\n",
            "  File \"/Users/ruihongyu/opt/anaconda3/lib/python3.8/site-packages/moviepy/video/VideoClip.py\", line 300, in write_videofile\n",
            "    ffmpeg_write_video(self, filename, fps, codec,\n",
            "  File \"/Users/ruihongyu/opt/anaconda3/lib/python3.8/site-packages/moviepy/video/io/ffmpeg_writer.py\", line 213, in ffmpeg_write_video\n",
            "    with FFMPEG_VideoWriter(filename, clip.size, fps, codec = codec,\n",
            "  File \"/Users/ruihongyu/opt/anaconda3/lib/python3.8/site-packages/moviepy/video/io/ffmpeg_writer.py\", line 88, in __init__\n",
            "    '-r', '%.02f' % fps,\n",
            "TypeError: must be real number, not NoneType\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Moviepy - Building video /Users/ruihongyu/Library/CloudStorage/OneDrive-ImperialCollegeLondon/Year_4/Reinforcement Learning/RL_CW_and_Lab/random_episode/random_episode.mp4.\n",
            "Moviepy - Writing video /Users/ruihongyu/Library/CloudStorage/OneDrive-ImperialCollegeLondon/Year_4/Reinforcement Learning/RL_CW_and_Lab/random_episode/random_episode.mp4\n",
            "\n",
            "Moviepy - Building video /Users/ruihongyu/Library/CloudStorage/OneDrive-ImperialCollegeLondon/Year_4/Reinforcement Learning/RL_CW_and_Lab/random_episode/random_episode.mp4.\n",
            "Moviepy - Writing video /Users/ruihongyu/Library/CloudStorage/OneDrive-ImperialCollegeLondon/Year_4/Reinforcement Learning/RL_CW_and_Lab/random_episode/random_episode.mp4\n",
            "\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "must be real number, not NoneType",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-f2895f8d39e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mvideo_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mvideo_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mvideo_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Closing video encoder: path={self.path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageSequenceClip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecorded_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes_per_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_videofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;31m# No frames captured. Set metadata.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \"\"\"\n\u001b[1;32m    230\u001b[0m     \u001b[0mevaldict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_call_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaller\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_func_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m     \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'_e%d_'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36mrequires_duration\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attribute 'duration' not set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \"\"\"\n\u001b[1;32m    230\u001b[0m     \u001b[0mevaldict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_call_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaller\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_func_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m     \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'_e%d_'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36muse_clip_fps_by_default\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m    133\u001b[0m              for (k,v) in k.items()}\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mnew_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \"\"\"\n\u001b[1;32m    230\u001b[0m     \u001b[0mevaldict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_call_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaller\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_func_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m     \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'_e%d_'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36mconvert_masks_to_RGB\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mismask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_RGB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/moviepy/video/VideoClip.py\u001b[0m in \u001b[0;36mwrite_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n\u001b[1;32m    298\u001b[0m                                        logger=logger)\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         ffmpeg_write_video(self, filename, fps, codec,\n\u001b[0m\u001b[1;32m    301\u001b[0m                            \u001b[0mbitrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbitrate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                            \u001b[0mpreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/moviepy/video/io/ffmpeg_writer.py\u001b[0m in \u001b[0;36mffmpeg_write_video\u001b[0;34m(clip, filename, fps, codec, bitrate, preset, withmask, write_logfile, audiofile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mlogfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Moviepy - Writing video %s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m     with FFMPEG_VideoWriter(filename, clip.size, fps, codec = codec,\n\u001b[0m\u001b[1;32m    214\u001b[0m                                 \u001b[0mpreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbitrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbitrate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                                 \u001b[0maudiofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maudiofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/moviepy/video/io/ffmpeg_writer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, size, fps, codec, audiofile, preset, bitrate, withmask, logfile, threads, ffmpeg_params)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;34m'-s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%dx%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;34m'-pix_fmt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rgba'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mwithmask\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'rgb24'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;34m'-r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'%.02f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;34m'-an'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-i'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         ]\n",
            "\u001b[0;31mTypeError\u001b[0m: must be real number, not NoneType"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
            "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
            "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
            "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
          ]
        }
      ],
      "source": [
        "# Creating the environment and a recorder to save a video in the './random_episode' folder\n",
        "# To save multiple videos, save each mp4 fle to a new directory\n",
        "\n",
        "cwd = os.getcwd()\n",
        "video_dir = os.path.join(cwd, 'random_episode')\n",
        "if not os.path.isdir(video_dir):\n",
        "    os.mkdir(video_dir)\n",
        "video_file = os.path.join(video_dir, \"random_episode.mp4\")\n",
        "env = gym.make('CartPole-v0', render_mode=\"rgb_array\")\n",
        "\n",
        "# Perform an episode in the environemnt with random actions\n",
        "state = env.reset()\n",
        "\n",
        "video_recorder = VideoRecorder(env, video_file, enabled=True)  #record a video of the episode\n",
        "done = False\n",
        "while not done:\n",
        "\n",
        "    video_recorder.capture_frame()\n",
        "    action = env.action_space.sample()  # sample a random possible action from the CartPole env\n",
        "    next_state, reward, done, truncated, info = env.step(action)\n",
        "    state = next_state\n",
        "\n",
        "video_recorder.capture_frame()\n",
        "video_recorder.close()\n",
        "video_recorder.enabled = False\n",
        "\n",
        "print(f\"Video saved in folder {video_dir}\")\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG1q0h4MuOvp"
      },
      "source": [
        "### Question 2: Implementing a simple hand-designed policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9o3d_CI_Pon1"
      },
      "outputs": [],
      "source": [
        "# [Action required]\n",
        "def simple_policy(state, p_random):\n",
        "    \"\"\"\n",
        "    Simple hand-crafted policy to act in the Cartpole environment.\n",
        "    Input:\n",
        "        - state {tensor} - current state of the environment\n",
        "        - p_random {float} - probability that the action is random\n",
        "    Output: action {int} - action to perform in the environemnt\n",
        "    \"\"\"\n",
        "    ####\n",
        "    # Add your code here\n",
        "    ####\n",
        "\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ys68tHGEPh7u"
      },
      "outputs": [],
      "source": [
        "# Rate of random action sampling\n",
        "p_random = 0.2\n",
        "\n",
        "# Performing an episode in the environemnt with simple policy\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    action = simple_policy(state, p_random)\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    state = next_state\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjtZS1qHUzHH"
      },
      "source": [
        "## Part 2: Introduction to PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7Tox87FUXwD"
      },
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GmBYBmriPSZ"
      },
      "outputs": [],
      "source": [
        "# Graphical class: this class modifies the original Gym class to be able to visualise your prediction\n",
        "# You DO NOT need to understand it to work on this lab assessment\n",
        "\n",
        "from gym.envs.classic_control.cartpole import CartPoleEnv\n",
        "from gym.wrappers.time_limit import TimeLimit\n",
        "\n",
        "class ShowCartPolePredictions(CartPoleEnv):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def step(self, state):\n",
        "        \"\"\"\n",
        "        Step takes the next state as input instead of action.\n",
        "        \"\"\"\n",
        "        self.state = state\n",
        "        x, x_dot, theta, theta_dot = state\n",
        "\n",
        "        done = bool(\n",
        "            x < -self.x_threshold\n",
        "            or x > self.x_threshold\n",
        "            or theta < -self.theta_threshold_radians\n",
        "            or theta > self.theta_threshold_radians\n",
        "                )\n",
        "\n",
        "        reward = 1.\n",
        "\n",
        "        return np.array(self.state, dtype=np.float32), reward, done, {}\n",
        "\n",
        "nb_path = os.path.join(os.getcwd(), 'tutorial_3_solution.ipynb')\n",
        "gym.envs.register(\n",
        "     id='ShowPredictionsCartPole',\n",
        "     entry_point=ShowCartPolePredictions,\n",
        "     max_episode_steps=500,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKHIOPm_UwK4"
      },
      "source": [
        "### Question 3: Understanding the MLP class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrrOYjZfcEIA"
      },
      "outputs": [],
      "source": [
        "# Multi Layer perceptron class\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size, num_hidden, hidden_size):\n",
        "        \"\"\"\n",
        "        Initialise the network.\n",
        "        Input:\n",
        "            - input_size {int} - size of input to the network\n",
        "            - output_size {int} - size of output to the network\n",
        "            - num_hidden {int} - number of hidden layers\n",
        "            - hidden_size {int} - size of each hidden layer\n",
        "        \"\"\"\n",
        "        super(MLP, self).__init__()\n",
        "        self.input_layer = nn.Linear(input_size, hidden_size) # First tranformation from the network input to the input of first hidden layer\n",
        "        self.hidden_layers = nn.ModuleList([nn.Linear(hidden_size, hidden_size) for _ in range(num_hidden-1)]) # All the hidden transformation\n",
        "        self.output_layer = nn.Linear(hidden_size, output_size) # Last tranformation from the last hidden layer output to the network output\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Get the output of the MLP.\n",
        "        Input: x {tensor} - one element or a batch of element\n",
        "        Ouput: y {tensor} - corresponding output\n",
        "        \"\"\"\n",
        "        x.to(device)\n",
        "        x = self.input_layer(x) # Passing through the input layer\n",
        "        x = F.relu(x) # Applying Relu activation\n",
        "        for layer in self.hidden_layers:\n",
        "          x = layer(x) # Passing through each hidden layer\n",
        "          x = F.relu(x) # Applying Relu activation\n",
        "        x = self.output_layer(x) # Passing through the output layer\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVEqgNkRwgCd",
        "outputId": "c6fe84c7-7a74-4343-d687-028af0c6a02e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The input is:\n",
            " tensor([0.0743, 0.7883, 0.0474, 0.7741, 0.5591, 0.3715, 0.8818, 0.3100, 0.1044,\n",
            "        0.0415])\n",
            "\n",
            "The correpsonding output is:\n",
            " tensor([ 0.0092,  0.2972, -0.2255, -0.1776, -0.2508,  0.1617,  0.1625, -0.0214,\n",
            "         0.0931,  0.2808], grad_fn=<AddBackward0>)\n",
            "\n",
            "The network has not been trained yet so this output is random.\n"
          ]
        }
      ],
      "source": [
        "# Initialise an MLP instance\n",
        "input_size = 10\n",
        "output_size = 10\n",
        "num_hidden = 3\n",
        "hidden_size = 15\n",
        "\n",
        "model = MLP(input_size, output_size, num_hidden, hidden_size)\n",
        "\n",
        "# Creating some false input\n",
        "x = torch.rand(10) # Random tensor\n",
        "print(\"The input is:\\n\", x)\n",
        "\n",
        "# Passing it through the network\n",
        "y = model.forward(x)\n",
        "print(\"\\nThe correpsonding output is:\\n\", y)\n",
        "print(\"\\nThe network has not been trained yet so this output is random.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lBcIUZ1x6Bg"
      },
      "source": [
        "### Question 4: Collecting data to train the state-predictor model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aydDmDSz3olA"
      },
      "outputs": [],
      "source": [
        "def batch_data(state_list, action_list, next_state_list, batch_size, num_batches):\n",
        "  \"\"\"\n",
        "  Reshape the data to match the model requirements.\n",
        "  Input:\n",
        "    - state_list {list of torch.tensor} - list of state encountered during all num_episode episodes\n",
        "    - action_list {list of torch.tensor} - list of action applied during all num_episode episodes\n",
        "    - next_state_list {list of torch.tensor} - list of next state each action lead to during all num_episode episodes\n",
        "    - batch_size {int} - number of steps in a batch\n",
        "    - num_batches {int} - total number of batches\n",
        "  Ouput:\n",
        "    - batched_state_action {torch.tensor} - input of the model of size (batch_size, 5)\n",
        "    - batched_next_state {torch.tensor} - target output of the model of size (batch_size, 4)\n",
        "  \"\"\"\n",
        "  # Reshape and concatenate the state and action (input of the network)\n",
        "  state_action_list = [torch.cat((torch.tensor(state_list[i]).float().unsqueeze(0), torch.tensor(action_list[i]).unsqueeze(0).unsqueeze(0)), dim=-1) for i in range(len(state_list))]\n",
        "  state_action = torch.cat(state_action_list)\n",
        "\n",
        "  # Reshape the next state\n",
        "  next_state = torch.cat([torch.tensor(next_state_list[i]).float().unsqueeze(0) for i in range(len(next_state_list))])\n",
        "\n",
        "  # Rearrange the data into batches\n",
        "  batched_state_action = [state_action[batch*batch_size:(batch+1)*batch_size] for batch in range(num_batches)]\n",
        "  batched_next_state = [next_state[batch*batch_size:(batch+1)*batch_size] for batch in range(num_batches)]\n",
        "\n",
        "  return batched_state_action, batched_next_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKfJjzV6UrfO"
      },
      "outputs": [],
      "source": [
        "# [Action required]\n",
        "def collect_data(num_episodes, p_random):\n",
        "  \"\"\"\n",
        "  Collect the data to train the predictor model.\n",
        "  Input:\n",
        "    - num_episode {int} - number of episodes to collect\n",
        "    - p_random {float} - probability used for the simple policy\n",
        "  Output:\n",
        "    - state_list {list of torch.tensor} - list of state encountered during all num_episode episodes\n",
        "    - action_list {list of torch.tensor} - list of action applied during all num_episode episodes\n",
        "    - next_state_list {list of torch.tensor} - list of next state each action lead to during all num_episode episodes\n",
        "  \"\"\"\n",
        "\n",
        "  # Containers for the data\n",
        "  state_list = [] # List of current states\n",
        "  action_list = [] # List of current actions\n",
        "  next_state_list = [] # List of next step states\n",
        "\n",
        "  # Creating the environment\n",
        "  env = gym.make('CartPole-v0')\n",
        "\n",
        "  ####\n",
        "  # Add your code here\n",
        "  # This is an example on how to fill state_list, action_list and next_state_list\n",
        "  # You would need to update it to collect enough data\n",
        "\n",
        "  state = env.reset()\n",
        "  action = simple_policy(state, p_random)\n",
        "  next_state, reward, done, _ = env.step(action)\n",
        "  state_list.append(state)\n",
        "  action_list.append(action)\n",
        "  next_state_list.append(next_state)\n",
        "\n",
        "  ####\n",
        "\n",
        "\n",
        "  # Closing the environment\n",
        "  env.close()\n",
        "\n",
        "  return state_list, action_list, next_state_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnZ5-DHlyDnD"
      },
      "outputs": [],
      "source": [
        "# Define parameters for the model\n",
        "num_episodes = 5000 # Total number of episodes collected in our dataset\n",
        "batch_size = 128 # Size of the batch to train the DNN\n",
        "p_random = 0.2 # Parameter of the simple_policy\n",
        "\n",
        "# Collect the data\n",
        "state_list, action_list, next_state_list = collect_data(num_episodes, p_random)\n",
        "num_batches = int(len(state_list)/batch_size)\n",
        "\n",
        "# Reshape them to match the model input/output\n",
        "batched_state_action, batched_next_state = batch_data(state_list, action_list, next_state_list, batch_size, num_batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bk-PIDvy3-o"
      },
      "source": [
        "### Question 5: Training a state predictor model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-7uYOAw6_3c"
      },
      "outputs": [],
      "source": [
        "def MSE_loss(prediction, target):\n",
        "  \"\"\"\n",
        "  MSE loss function.\n",
        "  Input:\n",
        "    - prediction {torch.tensor} - target\n",
        "    - target {torch.tensor} - model prediction\n",
        "  Output: loss {float} - MSE error between the prediction and the target\n",
        "  \"\"\"\n",
        "  return ((prediction - target)**2).sum(dim=-1).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "button": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyeJfvwXp3ta",
        "new_sheet": false,
        "outputId": "563d675a-a0e1-4c0f-8146-8d07f7f1e36b",
        "run_control": {
          "read_only": false
        },
        "scrolled": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f70e998bd10>]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOpklEQVR4nO3cf6jd9X3H8eeruTRrEUyi8UeN2bVVGHGDFg5K2QauaoyDNtL6h90fDVtL/lj9Y5VCUxzT2v6hbp2ltNsIbSEIa3SO0kApEm2FMYb1xDrarE1zjS0mVZuaIDipkvW9P+7X7Xg5Mffec+49OX6eDzjc8/1+P/fe98cLeeac742pKiRJ7XrbpAeQJE2WIZCkxhkCSWqcIZCkxhkCSWrczKQHWI7zzz+/ZmdnJz2GJE2VAwcO/LqqNi48P5UhmJ2dpd/vT3oMSZoqSX4x7LxvDUlS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS48YSgiTbkhxKMpdk15Dra5M80F1/PMnsguubk7yc5NPjmEeStHgjhyDJGuCrwI3AFuCjSbYsWPZx4GRVXQ7cB9yz4PrfA98ddRZJ0tKN4xXBVcBcVR2pqteAvcD2BWu2A3u65w8B1yYJQJKbgGeAg2OYRZK0ROMIwSXAswPHR7tzQ9dU1SngJeC8JOcAnwE+d6ZvkmRnkn6S/vHjx8cwtiQJJn+z+E7gvqp6+UwLq2p3VfWqqrdx48aVn0ySGjEzhq9xDLh04HhTd27YmqNJZoBzgReBq4Gbk9wLrAN+m+Q3VfWVMcwlSVqEcYTgCeCKJJcx/wf+LcCfLVizD9gB/AdwM/C9qirgj19fkORO4GUjIEmra+QQVNWpJLcCDwNrgG9U1cEkdwH9qtoHfB24P8kccIL5WEiSzgKZ/4v5dOn1etXv9yc9hiRNlSQHqqq38PykbxZLkibMEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS4wyBJDXOEEhS48YSgiTbkhxKMpdk15Dra5M80F1/PMlsd/76JAeS/Kj7+IFxzCNJWryRQ5BkDfBV4EZgC/DRJFsWLPs4cLKqLgfuA+7pzv8a+GBV/QGwA7h/1HkkSUszjlcEVwFzVXWkql4D9gLbF6zZDuzpnj8EXJskVfXDqvpld/4g8I4ka8cwkyRpkcYRgkuAZweOj3bnhq6pqlPAS8B5C9Z8BHiyql4dw0ySpEWamfQAAEmuZP7toq1vsmYnsBNg8+bNqzSZJL31jeMVwTHg0oHjTd25oWuSzADnAi92x5uAbwEfq6qnT/dNqmp3VfWqqrdx48YxjC1JgvGE4AngiiSXJXk7cAuwb8GafczfDAa4GfheVVWSdcB3gF1V9e9jmEWStEQjh6B7z/9W4GHgJ8CDVXUwyV1JPtQt+zpwXpI54Dbg9V8xvRW4HPibJE91jwtGnUmStHipqknPsGS9Xq/6/f6kx5CkqZLkQFX1Fp73XxZLUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuMMgSQ1zhBIUuPGEoIk25IcSjKXZNeQ62uTPNBdfzzJ7MC1z3bnDyW5YRzzSJIWb+QQJFkDfBW4EdgCfDTJlgXLPg6crKrLgfuAe7rP3QLcAlwJbAP+oft6kqRVMo5XBFcBc1V1pKpeA/YC2xes2Q7s6Z4/BFybJN35vVX1alU9A8x1X0+StErGEYJLgGcHjo9254auqapTwEvAeYv8XACS7EzST9I/fvz4GMaWJMEU3Syuqt1V1auq3saNGyc9jiS9ZYwjBMeASweON3Xnhq5JMgOcC7y4yM+VJK2gcYTgCeCKJJcleTvzN3/3LVizD9jRPb8Z+F5VVXf+lu63ii4DrgB+MIaZJEmLNDPqF6iqU0luBR4G1gDfqKqDSe4C+lW1D/g6cH+SOeAE87GgW/cg8F/AKeCTVfU/o84kSVq8zP/FfLr0er3q9/uTHkOSpkqSA1XVW3h+am4WS5JWhiGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMaNFIIkG5LsT3K4+7j+NOt2dGsOJ9nRnXtnku8k+WmSg0nuHmUWSdLyjPqKYBfwaFVdATzaHb9Bkg3AHcDVwFXAHQPB+Luq+j3gfcAfJrlxxHkkSUs0agi2A3u653uAm4asuQHYX1UnquoksB/YVlWvVNX3AarqNeBJYNOI80iSlmjUEFxYVc91z58HLhyy5hLg2YHjo925/5NkHfBB5l9VSJJW0cyZFiR5BLhoyKXbBw+qqpLUUgdIMgN8E/hyVR15k3U7gZ0AmzdvXuq3kSSdxhlDUFXXne5akheSXFxVzyW5GPjVkGXHgGsGjjcBjw0c7wYOV9WXzjDH7m4tvV5vycGRJA036ltD+4Ad3fMdwLeHrHkY2JpkfXeTeGt3jiRfAM4F/mrEOSRJyzRqCO4Grk9yGLiuOyZJL8nXAKrqBPB54InucVdVnUiyifm3l7YATyZ5KsknRpxHkrREqZq+d1l6vV71+/1JjyFJUyXJgarqLTzvvyyWpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMYZAklqnCGQpMaNFIIkG5LsT3K4+7j+NOt2dGsOJ9kx5Pq+JD8eZRZJ0vKM+opgF/BoVV0BPNodv0GSDcAdwNXAVcAdg8FI8mHg5RHnkCQt06gh2A7s6Z7vAW4asuYGYH9Vnaiqk8B+YBtAknOA24AvjDiHJGmZRg3BhVX1XPf8eeDCIWsuAZ4dOD7anQP4PPBF4JUzfaMkO5P0k/SPHz8+wsiSpEEzZ1qQ5BHgoiGXbh88qKpKUov9xkneC7ynqj6VZPZM66tqN7AboNfrLfr7SJLe3BlDUFXXne5akheSXFxVzyW5GPjVkGXHgGsGjjcBjwHvB3pJft7NcUGSx6rqGiRJq2bUt4b2Aa//FtAO4NtD1jwMbE2yvrtJvBV4uKr+sareVVWzwB8BPzMCkrT6Rg3B3cD1SQ4D13XHJOkl+RpAVZ1g/l7AE93jru6cJOkskKrpe7u91+tVv9+f9BiSNFWSHKiq3sLz/stiSWqcIZCkxhkCSWqcIZCkxhkCSWqcIZCkxhkCSWqcIZCkxhkCSWqcIZCkxhkCSWqcIZCkxhkCSWqcIZCkxhkCSWqcIZCkxhkCSWqcIZCkxhkCSWqcIZCkxhkCSWqcIZCkxhkCSWqcIZCkxqWqJj3DkiU5Dvxi0nMs0fnAryc9xCpzz21wz9Pjd6tq48KTUxmCaZSkX1W9Sc+xmtxzG9zz9POtIUlqnCGQpMYZgtWze9IDTIB7boN7nnLeI5CkxvmKQJIaZwgkqXGGYIySbEiyP8nh7uP606zb0a05nGTHkOv7kvx45Sce3Sh7TvLOJN9J8tMkB5PcvbrTL02SbUkOJZlLsmvI9bVJHuiuP55kduDaZ7vzh5LcsJpzj2K5e05yfZIDSX7UffzAas++HKP8jLvrm5O8nOTTqzXzWFSVjzE9gHuBXd3zXcA9Q9ZsAI50H9d3z9cPXP8w8M/Ajye9n5XeM/BO4E+6NW8H/g24cdJ7Os0+1wBPA+/uZv1PYMuCNX8J/FP3/Bbgge75lm79WuCy7uusmfSeVnjP7wPe1T3/feDYpPezkvsduP4Q8C/Apye9n6U8fEUwXtuBPd3zPcBNQ9bcAOyvqhNVdRLYD2wDSHIOcBvwhVWYdVyWveeqeqWqvg9QVa8BTwKbVmHm5bgKmKuqI92se5nf+6DB/xYPAdcmSXd+b1W9WlXPAHPd1zvbLXvPVfXDqvpld/4g8I4ka1dl6uUb5WdMkpuAZ5jf71QxBON1YVU91z1/HrhwyJpLgGcHjo925wA+D3wReGXFJhy/UfcMQJJ1wAeBR1diyDE44x4G11TVKeAl4LxFfu7ZaJQ9D/oI8GRVvbpCc47Lsvfb/SXuM8DnVmHOsZuZ9ADTJskjwEVDLt0+eFBVlWTRv5ub5L3Ae6rqUwvfd5y0ldrzwNefAb4JfLmqjixvSp2NklwJ3ANsnfQsK+xO4L6qerl7gTBVDMESVdV1p7uW5IUkF1fVc0kuBn41ZNkx4JqB403AY8D7gV6SnzP/c7kgyWNVdQ0TtoJ7ft1u4HBVfWkM466UY8ClA8ebunPD1hzt4nYu8OIiP/dsNMqeSbIJ+Bbwsap6euXHHdko+70auDnJvcA64LdJflNVX1n5scdg0jcp3koP4G95443Te4es2cD8+4jru8czwIYFa2aZnpvFI+2Z+fsh/wq8bdJ7OcM+Z5i/yX0Z/38j8coFaz7JG28kPtg9v5I33iw+wnTcLB5lz+u69R+e9D5WY78L1tzJlN0snvgAb6UH8++NPgocBh4Z+MOuB3xtYN1fMH/DcA748yFfZ5pCsOw9M/83rgJ+AjzVPT4x6T29yV7/FPgZ879Zcnt37i7gQ93z32H+N0bmgB8A7x743Nu7zzvEWfqbUePcM/DXwH8P/FyfAi6Y9H5W8mc88DWmLgT+LyYkqXH+1pAkNc4QSFLjDIEkNc4QSFLjDIEkNc4QSFLjDIEkNe5/AecL/ch2b2HBAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Creating the environment\n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "# Defining the parameters\n",
        "state_dim = 4\n",
        "action_dim = 1\n",
        "\n",
        "input_size = state_dim + action_dim\n",
        "output_size = state_dim\n",
        "num_hidden = 2\n",
        "hidden_size = 50\n",
        "\n",
        "# Creating the predictor model\n",
        "state_predictor = MLP(input_size, output_size, num_hidden, hidden_size)\n",
        "\n",
        "# Creating the optmizer\n",
        "optimiser = optim.Adam(state_predictor.parameters())\n",
        "\n",
        "# [Action required]\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "losses = [] # Contain all successive loss function values\n",
        "\n",
        "####\n",
        "# Add your code here\n",
        "####\n",
        "\n",
        "# Closing the environment\n",
        "env.close()\n",
        "\n",
        "# Plot the loss across training\n",
        "plt.plot(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sK7wN8a9GKC"
      },
      "outputs": [],
      "source": [
        "# Displaying the learned model dynamics in the CartPole environment\n",
        "\n",
        "# simulated_env allows us to visualise the learned model dynamics\n",
        "# by calling simulated_env.set_next_state(next_state) we set the learned\n",
        "# next_state and we can visualise what the learned dynamics looks like\n",
        "# video saved in the 'learned_dynamics' folder\n",
        "\n",
        "# setting up the visualisation\n",
        "\n",
        "cwd = os.getcwd()\n",
        "video_dir = os.path.join(cwd, 'learned_dynamics')\n",
        "if not os.path.isdir(video_dir):\n",
        "    os.mkdir(video_dir)\n",
        "video_file = os.path.join(video_dir, \"learned_dynamics.mp4\")\n",
        "\n",
        "simulated_env = gym.make('ShowPredictionsCartPole')\n",
        "#TimeLimit)(ShowCartPolePredictions(), max_episode_steps=500)\n",
        "video_recorder = VideoRecorder(simulated_env, video_file, enabled=True)\n",
        "\n",
        "state = simulated_env.reset()\n",
        "\n",
        "#Performing the episode\n",
        "state = simulated_env.reset()\n",
        "done = False\n",
        "state = torch.tensor(simulated_env.state).float()\n",
        "\n",
        "while not done:\n",
        "\n",
        "    # Store the current state of the CartPole for the video\n",
        "    video_recorder.capture_frame()\n",
        "\n",
        "    # Predict the state with the model\n",
        "    action = torch.tensor([simple_policy(state, p_random)])\n",
        "    state_action = torch.cat((state, action))\n",
        "    with torch.no_grad():\n",
        "        predicted_state = state_predictor(state_action)\n",
        "        predicted_state = list([float(s) for s in predicted_state.squeeze()])\n",
        "\n",
        "    # Apply it in the environment\n",
        "    state, reward, done, info = simulated_env.step(predicted_state)\n",
        "    state = torch.tensor(simulated_env.state).float()\n",
        "\n",
        "video_recorder.capture_frame()\n",
        "video_recorder.close()\n",
        "video_recorder.enabled = False\n",
        "\n",
        "simulated_env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLriyAW593AR"
      },
      "source": [
        "### Question 6: Trying multiple loss functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbHSF0Bw934N"
      },
      "outputs": [],
      "source": [
        "# Alternative loss function\n",
        "\n",
        "def L1_loss(prediction, target):\n",
        "  \"\"\"\n",
        "  L1 loss function.\n",
        "  Input:\n",
        "    - prediction {torch.tensor} - target\n",
        "    - target {torch.tensor} - model prediction\n",
        "  Output: loss {float} - L1 error between the prediction and the target\n",
        "  \"\"\"\n",
        "  return (abs(prediction - target)).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "ru5SHJDI-HDB",
        "outputId": "2899fffb-f02e-4deb-dd57-0e0f722e93a6"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-13f7adf1719b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# You can call it the same way you would do with our hand design loss:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatched_state_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Input of the model for this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatched_next_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Target output of the model for this batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_MSE_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "## Torch MSE loss function\n",
        "torch_MSE_loss = nn.MSELoss()\n",
        "\n",
        "# You can call it the same way you would do with our hand design loss:\n",
        "inputs = batched_state_action[0] # Input of the model for this batch\n",
        "targets = batched_next_state[0] # Target output of the model for this batch\n",
        "loss = torch_MSE_loss(state_predictor(inputs), targets)\n",
        "print(loss)\n",
        "\n",
        "## Torch L1 loss function\n",
        "torch_L1_loss = nn.L1Loss()\n",
        "\n",
        "# You can call it the same way you would do with our hand design loss:\n",
        "inputs = batched_state_action[0] # Input of the model for this batch\n",
        "targets = batched_next_state[0] # Target output of the model for this batch\n",
        "loss = torch_L1_loss(state_predictor(inputs), targets)\n",
        "print(loss)\n",
        "\n",
        "## Torch Huber loss function\n",
        "torch_Huber_loss = nn.HuberLoss()\n",
        "\n",
        "# You can call it the same way you would do with our hand design loss:\n",
        "inputs = batched_state_action[0] # Input of the model for this batch\n",
        "targets = batched_next_state[0] # Target output of the model for this batch\n",
        "loss = torch_Huber_loss(state_predictor(inputs), targets)\n",
        "print(loss)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "pl3dm2KOPrfi",
        "VG1q0h4MuOvp",
        "B7Tox87FUXwD",
        "NKHIOPm_UwK4",
        "6lBcIUZ1x6Bg",
        "0Bk-PIDvy3-o",
        "uLriyAW593AR"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
